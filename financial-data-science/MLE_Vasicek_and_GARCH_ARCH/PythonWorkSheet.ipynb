{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Worksheet: Maximum Likelihood for a Nonlinear Problem\n",
    "\n",
    "Dear students,\n",
    "\n",
    "I am sorry for handing out the worksheet so late. My team and I tried hard to get a useful exercise for MLE. MLE is a crucial tool for all data science. In addition, our standard problem set was handed out as part of the python lecture notes. We hence had to come up with something useful, yet, also doable at the BSc level.\n",
    "\n",
    "The following exercise is the result. We will likely spend some time discussing your and my work during the upcoming prof cafe.\n",
    "\n",
    "\n",
    "If a significant number of learning groups communicate back that the task is too hard to accomplish till monday, I am willing to extend the final submission by one week. Yet, there will be (a then shorter) PS on vol modeling ARCH due in the same week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember this Crucial FACT\n",
    "\n",
    "**Do NOT numerically optimize a Gaussian likelihood for a LINEAR model. Instead, take the ML parameter estimates from an OLS fit AND the ML estimate for the volatility from the respective residual**\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{T} \\epsilon_{ols}' \\epsilon_{ols}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A Useful Exercise had to be Non-Linear\n",
    "\n",
    "Reason: see above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.0 Set-up of the Python Challenge\n",
    "\n",
    "Have you ever wondered how central banks, banks, hedge funds and investors know whether a government bond is fairly priced, overpriced or undervalued? Well, they use the approach of \"net present value\" to figure out what the price of a bond should be. To do so, one has to get accurate forecasts of how the short rate and risk premia move into the future. A popular approach relies on so called \"short rate models\". These models fit the time series of the short rate to a realistic, yet convenient, parametrization. This allows to obtain a forecast of where short rates will be in the future. You need to do the same for the risk premium. You might wonder why do government bonds pay a risk premium. Well, the longer the maturity of the bond, the higher the interest rate sensitivity and hence, the higher (usually) the (maturity) premium. That premium compensates for the fact that future realized interest rates will be different from todays expected future rates. Based on the CAPM, you could say that interest rate risk is systematic and hence pays a risk premium. That is for the background.\n",
    "\n",
    "A very popular short rate model is called \"Vasicek model\". That is because a mathematician, Dr. Vasicek, derived in the 1980s the term structure of interest rates under the assumption that the short rate follows a Gaussian distribution. It is fair to says that the assumed short rate follows a G-LRM. Based on stochastic calculus and the principle of net present value and no arbitrage, Dr. Vasicek derived the price of risk-free (non defaultable) bonds for different maturities. \n",
    "\n",
    "The Gaussian PDF of the short rate is\n",
    "\n",
    "$$\n",
    "r_t | F_{t-1} \\sim \\mathcal N\\left(\\theta^P (1- e^{-\\kappa^P})+e^{-\\kappa^P} r_{t-1}, \\sigma^2 * \\frac{1-e^{-2\\kappa^P}}{2\\kappa^P}\\right).\n",
    "$$\n",
    "\n",
    "Note, $\\theta^P$ is the long-run mean of $r$, $kappa^P$ is the speed of mean reversion and $\\sigma$ is the instantaneous standard deviation of the interest rate shock. As the model of Vasicek is written in continuous time, the above PDF takes several exponentials and ratios, which are the result of an integration that is NOT subject to our BSc course.\n",
    "\n",
    "Also, Dr. Vasicek derived that the arbitrage-free price of a bond at time $t$ with maturity in $\\tau$ periods, $P_t(\\tau)$ coincides with\n",
    "\n",
    "$$\n",
    "P_t(\\tau) = e^{A(\\tau) - B(\\tau) \\times r_t}\n",
    "$$\n",
    "\n",
    "with\n",
    "\\begin{align*}\n",
    "B(\\tau) &:= \\frac{1-e^{-\\kappa^Q \\tau}}{\\kappa^Q} \\\\\n",
    "A(\\tau) &:= (\\theta^Q - \\frac{\\sigma^2}{2 (\\kappa^Q)^2}) \\times (  B(\\tau) - \\tau ) - \\frac{\\sigma^2}{4\\kappa^Q} B^2(\\tau) \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Note, from the CAPM and NPV you know that prices of financial assets are discounted by the risk-free rate plus risk premium. The parameters $\\theta^Q$ and $\\kappa^Q$ differ from $\\theta^P$ and $\\kappa^P$, respectively. The former are adjusted by risk premiums. Intuitively, it is correct to think that the former are risk premium adjusted expressions of the latter. E.g. if the long-run mean of the short rate was at 2\\%, it could be that bonds are priced as if the long-run rate was at 3\\%. This means the price is lower than if the future payoff was discounted by 2\\%. That extra reduction in price can be correctly interpreted as the result of a risk premium. That was to show you that the so called \"Q parameters\" are risk premium adjusted (and extracted from prices), while the $P$ parameters are extracted from empirical time-series data. I provide a formal and intuitive introduction into these asset pricing concepts in an upcoming MSc class on financial machine learning.\n",
    "\n",
    "For now, you need only to keep in mind that the parameters of the model are $\\kappa^P, \\theta^P$ for the short rate PDF and $\\theta^Q, \\kappa^Q$ for the bond prices. The parameter $\\sigma$ affects both, the short rate PDF and the bond price PDF.\n",
    "\n",
    "Note, prices are non-stationary! Hence, we work with continuousy compounded yields\n",
    "\\begin{align*}\n",
    "y_t(\\tau) &:= - \\frac{1}{\\tau} \\ln P_t(\\tau)  \n",
    "\\end{align*}\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.1 Data\n",
    "\n",
    "You will work with the same interest rate data than a couple of weeks ago. The rates are in \"GovBondYields.xls\". \n",
    "\n",
    "Take the 12-, 60 and 120 months interest rates as your yields of interest, $y$. Take the 3-month yield as the short rate $r$.\n",
    "\n",
    "**Units:** Divide the rates data by 1200 to arrive at monthly values in decimals. Work with these units.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.2 Financial Data Science Challenge\n",
    "\n",
    "Your data science task is to extract the time-series of the expected market price of risk that is priced into bonds. You can consider the latter to be the priced-in Expected Sharpe Ratio in bonds (expected risk premium per unit of risk). The precise formula is given below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.3 Battle Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The battle plan is as follows. \n",
    "\n",
    "1. Write down the joint log likelihood function for $r_t$ and $y_t(\\tau), \\tau \\in [12,60,120]$. For that assume a pairwise independent Gaussian measurement error for all yields. Further assume each of these measurement errors shares the same volatility, which we call\n",
    "$$\n",
    "\\sigma_y.\n",
    "$$\n",
    "Note, $\\sigma_y$ will be part of the parameter vector over which you optimize.\n",
    "\n",
    "Note, you can interpret observed yields as Vasicek (model)-implied yield $y_t(\\tau)$ plus a Gaussian measurement error that is drawn from $N(0,\\sigma^2_y)$.\n",
    "\n",
    "Note: the above assumption implies that the joint log likelihood function can be treated as the sum of 4 pairwise independent log likelihood functions. The independence refers to the respective residual being independent of the residuals of the other log likelihood. The parameters do of course affect all of the log likelihoods; although strictly speaking, $\\kappa^P, \\theta^P$ affect only the log likelihood for $r$, while $\\kappa^Q, \\theta^Q$ affect only the log likelhood for the 3 yields.\n",
    "\n",
    "2. Write a python function that implements the joint log likelihood function. More recent research has shown you could indeed fit the PDF for $r$ independently for the PDF of $y$. Yet, here, we optimize the joint log likelihood.\n",
    "\n",
    "3. Play a bit around with starting values for the optimization. That simple example reveals already, that you need to choose starting values carefully to end up in the global optimum. Regardless of your playing around, use at least the following procedure to find smart starting values\n",
    "\n",
    "3.1 fit $r$ to an OLS and recover the ML estimates for $\\kappa^P, \\theta^P, \\sigma$. Make these parameters be the respective starting values $\\kappa^P, \\theta^P, \\sigma$ in  the joint log likelihood optimization. \n",
    "\n",
    "3.2 for the starting values of the Q parameters, we assume a 0 risk premium. Hence $\\kappa^Q_{startValue} = \\kappa^P_{startValue}$ and $\\theta^Q_{startValue} = \\theta^P_{startValue}$. Double check: inside the optimization, Q and P parameters can differ(!). P parameters affect $r$ while $Q$ parameters affect yields (!)\n",
    "\n",
    "3.3 For the starting value of $\\sigma_y$ we do the following: Regress (OLS) the 60month yield onto $r$ and take the volatility of the residual as the starting value for $\\sigma_y$. \n",
    "\n",
    "\n",
    "4. The optimization routine is similar to the ipynb from class\n",
    "\n",
    "4.1 use sco.minimize\n",
    "\n",
    "4.2 use L-BFGS-B as the optimization routine\n",
    "\n",
    "4.3 use a tol of 1e-8\n",
    "\n",
    "4.5 minimize the negative joint log likelihood function\n",
    "\n",
    "4.6 the upper bound for all parameters is 100. \n",
    "\n",
    "4.7 the lower bound for all parameters is -100, except for $\\sigma$ and $\\sigma_y$ who lower bound we set to 0.000001.\n",
    "\n",
    "5. Estimate the model parameters of Vasicek.\n",
    "\n",
    "6. Inside the vasicek model, the priced-in market price of risk (Sharpe ratio) in bonds is\n",
    "\n",
    "$$\n",
    "RP_t = \\lambda_0 + \\lambda_1 \\times r_t\n",
    "$$\n",
    "\n",
    "with\n",
    "\\begin{align*}\n",
    "\\lambda_0 &:= \\frac{\\kappa^P \\theta^P - \\kappa^Q \\theta^Q}{\\sigma_r} \\\\\n",
    "\\lambda_1 &:= \\frac{\\kappa^Q - \\kappa^P}{\\sigma_r}.\n",
    "\\end{align*}\n",
    "\n",
    "Here, you see that spread of Q and P parameters captures risk premium information. A formal explanation of that is not doable in the BSc course on Financial Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('GovBondYields.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Date', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df/1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df[3]\n",
    "y_12 = df[12]\n",
    "y_60 = df[60]\n",
    "y_120 = df[120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression to arrive at starting values for $\\kappa^P$ and $\\theta^P$ and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog = r[1:]\n",
    "exog = r[:-1].values\n",
    "exog = sm.add_constant(exog)\n",
    "model = sm.OLS(endog=endog, exog=exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      3   R-squared:                       0.974\n",
      "Model:                            OLS   Adj. R-squared:                  0.974\n",
      "Method:                 Least Squares   F-statistic:                 2.339e+04\n",
      "Date:                Fri, 11 Jun 2021   Prob (F-statistic):               0.00\n",
      "Time:                        14:11:51   Log-Likelihood:                 4033.2\n",
      "No. Observations:                 624   AIC:                            -8062.\n",
      "Df Residuals:                     622   BIC:                            -8054.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        6.89e-05   3.17e-05      2.170      0.030    6.56e-06       0.000\n",
      "x1             0.9852      0.006    152.940      0.000       0.973       0.998\n",
      "==============================================================================\n",
      "Omnibus:                      289.299   Durbin-Watson:                   1.321\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            14612.650\n",
      "Skew:                          -1.274   Prob(JB):                         0.00\n",
      "Kurtosis:                      26.570   Cond. No.                         426.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_kappa_P = - np.log(res.params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014895332457501307"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_kappa_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_theta_P = res.params[0]/(1-res.params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004660456741451862"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_theta_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_sigma = np.sqrt(res.mse_resid*2*ols_kappa_P/(1-np.exp(-2*ols_kappa_P)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00038075877416762224"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_r_params = [ols_theta_P, ols_kappa_P, ols_sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating est_sigma_y\n",
    "endog = y_60.values\n",
    "exog = r\n",
    "exog = sm.add_constant(exog)\n",
    "model = sm.OLS(endog=endog, exog=exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.885\n",
      "Model:                            OLS   Adj. R-squared:                  0.885\n",
      "Method:                 Least Squares   F-statistic:                     4811.\n",
      "Date:                Fri, 11 Jun 2021   Prob (F-statistic):          3.27e-295\n",
      "Time:                        15:11:12   Log-Likelihood:                 3589.2\n",
      "No. Observations:                 625   AIC:                            -7174.\n",
      "Df Residuals:                     623   BIC:                            -7166.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0013   6.52e-05     20.597      0.000       0.001       0.001\n",
      "3              0.9186      0.013     69.364      0.000       0.893       0.945\n",
      "==============================================================================\n",
      "Omnibus:                       13.776   Durbin-Watson:                   0.107\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               14.299\n",
      "Skew:                           0.371   Prob(JB):                     0.000785\n",
      "Kurtosis:                       3.000   Cond. No.                         426.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_sigma_y = np.sqrt(res.mse_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007770131789906451"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_sigma_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write down the joint log likelihood function for $r_t$ and $y_t(\\tau), \\tau \\in [12,60,120]. \\sigma_y $ is assumed to be homoscedastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(tau, kappa_Q):\n",
    "    return (1-np.exp(-kappa_Q*tau))/kappa_Q\n",
    "\n",
    "def A(tau, theta_Q, kappa_Q, sigma):\n",
    "    result =  (theta_Q - (sigma**2)/(2*kappa_Q**2))*(B(tau, kappa_Q) - tau) - (sigma**2)/(4*kappa_Q)*B(B(tau, kappa_Q), kappa_Q) # (??) B^2(tau) or B(tau)^2\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def vasicek_bond_price(tau, theta_Q, kappa_Q, sigma, r_t):\n",
    "    P_t = np.exp(A(tau, theta_Q, kappa_Q, sigma) - B(tau, kappa_Q)*r_t)\n",
    "    return P_t\n",
    "\n",
    "def vasicek_bond_yield(tau, theta_Q, kappa_Q, sigma, r_t):\n",
    "    P_t = vasicek_bond_price(tau, theta_Q, kappa_Q, sigma, r_t)\n",
    "    vbq = -np.log(P_t)/tau\n",
    "    return vbq\n",
    "\n",
    "def r_log_likelihood(theta_P, kappa_P, sigma, sigma_y):\n",
    "    endog = r[1:]\n",
    "    exog = r[:-1]\n",
    "    log_likelihood = 0\n",
    "    for r_tm1, r_t in zip(exog, endog):\n",
    "        distr = stats.norm(theta_P*(1-np.exp(-kappa_P))+np.exp(-kappa_P)*r_tm1, sigma_y + np.sqrt(sigma**2*(1-np.exp(-2*kappa_P))/(2*kappa_P)))\n",
    "        log_likelihood_t = np.log(distr.pdf(r_t))\n",
    "        log_likelihood += log_likelihood_t\n",
    "    return log_likelihood\n",
    "\n",
    "def neg_log_likelihood(params):\n",
    "    print(params)\n",
    "    # Q's are for y P's are for r \n",
    "    theta_Q, kappa_Q, theta_P, kappa_P, sigma, sigma_y = params\n",
    "    \n",
    "    # yield likelihood\n",
    "    wnp = stats.norm(0, sigma_y)\n",
    "    vby_12 = r.apply(lambda x: vasicek_bond_yield(12, theta_Q, kappa_Q, sigma, x))\n",
    "    vby_60 = r.apply(lambda x: vasicek_bond_yield(60, theta_Q, kappa_Q, sigma, x))\n",
    "    vby_120 = r.apply(lambda x: vasicek_bond_yield(120, theta_Q, kappa_Q, sigma, x))\n",
    "    vby_log_likelihood = sum([np.log(wnp.pdf(np.abs(x - z))).sum() for x, z in [(y_12, vby_12), (y_60, vby_60), (y_120, vby_60)]])\n",
    "    #print(f\"vby_log_likelihood: {vby_log_likelihood}\")\n",
    "    \n",
    "    # short rate likelihood\n",
    "    short_rate_log_likelihood = r_log_likelihood(theta_P, kappa_P, sigma, sigma_y)\n",
    "    #print(f\"short_rate_log_likelihood: {short_rate_log_likelihood}\")\n",
    "    \n",
    "    overall_log_likelihood = vby_log_likelihood + short_rate_log_likelihood\n",
    "    print(f\"overall_log_likelihood: {overall_log_likelihood}\")\n",
    "    return -overall_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.004660456741451862, 0.014895332457501307, 0.00038075877416762224]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_r_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-278-34ed90086cff>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (1-np.exp(-kappa_Q*tau))/kappa_Q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A(12, 10, -10, ols_sigma) == np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1954-04-01   NaN\n",
       "1954-05-01   NaN\n",
       "1954-06-01   NaN\n",
       "1954-07-01   NaN\n",
       "1954-08-01   NaN\n",
       "              ..\n",
       "2005-12-01   NaN\n",
       "2006-01-01   NaN\n",
       "2006-02-01   NaN\n",
       "2006-03-01   NaN\n",
       "2006-04-01   NaN\n",
       "Name: 3, Length: 625, dtype: float64"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnp = stats.norm(0, est_sigma_y)\n",
    "z = r.apply(lambda x: vasicek_bond_price(12, 100, -100, 0.00038075877416762224, x))\n",
    "z\n",
    "#wnp.pdf(np.abs(r-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = np.array([100, -100, ols_theta_P, ols_kappa_P, ols_sigma, est_sigma_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+02 -1.00000000e+02  4.66045674e-03  1.48953325e-02\n",
      "  3.80758774e-04  7.77013179e-04]\n",
      "vby_log_likelihood: nan\n",
      "overall_log_likelihood: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_log_likelihood(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a python function that implements the joint log likelihood function. More recent research has shown you could indeed fit the PDF for $r$ independently for the PDF of $y$. Yet, here, we optimize the joint log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make bounds for parameters:\n",
    "bounds = ((-0.5, 0.5),\n",
    "         (-2, 5),\n",
    "         (-2, 5),\n",
    "         (-2, 5),\n",
    "         (0.000001, 10),\n",
    "         (0.000001, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as sco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = np.array([ols_theta_P, \n",
    "                        ols_kappa_P, \n",
    "                        ols_theta_P, \n",
    "                        ols_kappa_P, \n",
    "                        ols_sigma, \n",
    "                        est_sigma_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00466046, 0.01489533, 0.00466046, 0.01489533, 0.00038076,\n",
       "       0.00077701])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00489348 0.0156401  0.00489348 0.0156401  0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.90597717192\n",
      "[0.00489349 0.0156401  0.00489348 0.0156401  0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.91289674141\n",
      "[0.00489348 0.01564011 0.00489348 0.0156401  0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.905891999782\n",
      "[0.00489348 0.0156401  0.00489349 0.0156401  0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.905976917915\n",
      "[0.00489348 0.0156401  0.00489348 0.01564011 0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.905978063029\n",
      "[0.00489348 0.0156401  0.00489348 0.0156401  0.00039981 0.00081586]\n",
      "overall_log_likelihood: 13070.890217539294\n",
      "[0.00489348 0.0156401  0.00489348 0.0156401  0.0003998  0.00081587]\n",
      "overall_log_likelihood: 13070.93046935586\n",
      "[ 5.e-01 -2.e+00 -2.e+00  5.e+00  1.e-06  1.e+01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-278-34ed90086cff>:38: RuntimeWarning: divide by zero encountered in log\n",
      "  vby_log_likelihood = sum([np.log(wnp.pdf(np.abs(x - z))).sum() for x, z in [(y_12, vby_12), (y_60, vby_60), (y_120, vby_60)]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_log_likelihood: -inf\n",
      "[ 4.9999999e-01 -2.0000000e+00 -2.0000000e+00  5.0000000e+00\n",
      "  1.0000000e-06  1.0000000e+01]\n",
      "overall_log_likelihood: -inf\n",
      "[ 5.00000000e-01 -1.99999999e+00 -2.00000000e+00  5.00000000e+00\n",
      "  1.00000000e-06  1.00000000e+01]\n",
      "overall_log_likelihood: -inf\n",
      "[ 5.00000000e-01 -2.00000000e+00 -1.99999999e+00  5.00000000e+00\n",
      "  1.00000000e-06  1.00000000e+01]\n",
      "overall_log_likelihood: -inf\n",
      "[ 5.00000000e-01 -2.00000000e+00 -2.00000000e+00  4.99999999e+00\n",
      "  1.00000000e-06  1.00000000e+01]\n",
      "overall_log_likelihood: -inf\n",
      "[ 5.00e-01 -2.00e+00 -2.00e+00  5.00e+00  1.01e-06  1.00e+01]\n",
      "overall_log_likelihood: -inf\n",
      "[ 5.00000000e-01 -2.00000000e+00 -2.00000000e+00  5.00000000e+00\n",
      "  1.00000000e-06  9.99999999e+00]\n",
      "overall_log_likelihood: -inf\n",
      "[0.00489348 0.0156401  0.00489348 0.0156401  0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.90597717192\n",
      "[0.00489349 0.0156401  0.00489348 0.0156401  0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.91289674141\n",
      "[0.00489348 0.01564011 0.00489348 0.0156401  0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.905891999782\n",
      "[0.00489348 0.0156401  0.00489349 0.0156401  0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.905976917915\n",
      "[0.00489348 0.0156401  0.00489348 0.01564011 0.0003998  0.00081586]\n",
      "overall_log_likelihood: 13070.905978063029\n",
      "[0.00489348 0.0156401  0.00489348 0.0156401  0.00039981 0.00081586]\n",
      "overall_log_likelihood: 13070.890217539294\n",
      "[0.00489348 0.0156401  0.00489348 0.0156401  0.0003998  0.00081587]\n",
      "overall_log_likelihood: 13070.93046935586\n"
     ]
    }
   ],
   "source": [
    "MLE = sco.minimize(fun = neg_log_likelihood, \n",
    "                   x0 = init_params*1.05, \n",
    "                   bounds=bounds, \n",
    "                   method='L-BFGS-B', \n",
    "                   tol=1e-8, \n",
    "                   options={'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -13070.90597717192\n",
       " hess_inv: <6x6 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-6.91956949e+05,  8.51721379e+03,  2.54005499e+01, -8.91108357e+01,\n",
       "        1.57596326e+06, -2.44921839e+06])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 21\n",
       "      nit: 1\n",
       "     njev: 3\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.00489348, 0.0156401 , 0.00489348, 0.0156401 , 0.0003998 ,\n",
       "       0.00081586])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Play a bit around with starting values for the optimization. That simple example reveals already, that you need to choose starting values carefully to end up in the global optimum. Regardless of your playing around, use at least the following procedure to find smart starting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
